{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oats or Die! \n",
    "*A delightful life and death lesson in reinforcement learning*\n",
    "\n",
    "To skip right to the code [click here](#letsBegin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll do\n",
    "1. You will discover Q-learning. \n",
    "1. You will learn about the \"Gym\".\n",
    "    * The [OpenAI Gym](https://gym.openai.com/) is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball.\n",
    "    * There are many like it, but this one is mine. ... My gym, without me, is useless.\n",
    "1. You will train an Agent to find the optimal solution to the Gym - Taxi Environment\n",
    "    * There will be more details on the Taxi environment later, but here's a peek at the environment in all of its ASCII glory:\n",
    "    \n",
    "<img src=\"images/taxi-v2sm.png\" alt=\"TaxiV2\" height=\"100\" style=\"float:left\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning \n",
    "\n",
    "The 'Q' stands for quality, or, what is the *quality of the reward signal* for being in a particular state.  Our agent's goal is to maximize the reward signal: the Reward Hypothesis.  Each state in the environment will have a reward attached to it.  This reward can be positive or negative. Q-Learning is the oldest most basic form of RL and belongs to what is known as \"Tabular Solution Methods\".\n",
    "\n",
    "A few more items of note before we jump into Q-Learning:\n",
    "\n",
    "* Agents are disposable.  Your agent will \"die\" many deaths as it explores the environment.\n",
    "* Each time an agent \"dies\" or completes the task it is called an \"episode\".\n",
    "* Many timesteps make up an episode.  The number of timesteps per episode will vary. Although it is called a timestep there is no specific or set amount of \"wall time\" associated with each timestep.  \n",
    "\n",
    "### One complete timestep cycle\n",
    "\n",
    "* The **agent** makes an observation which means it receives the state of the observable portion of the **environment**, \\\\(s_t \\\\): **state** at timestep 't'. \n",
    "* Based on the observed state the agent selects a discrete action from the action space. A discrete action means the action you select is not dependent on any other actions. The **action** space is a finite list of actions your agent can choose. This list of actions is managed by the environment, \\\\(a_t \\\\): action at timestep 't'.\n",
    "* The agent takes the selected action and receives the reward signal, \\\\(r_t \\\\), associated with that action in that state: **reward** at timestep 't': \n",
    "\n",
    "These three steps form the basis for the agent-environment interaction:\n",
    "<img src=\"images/agent-enviro-interaction.png\" alt=\"agent-enviro\" width=\"400\" />\n",
    "\n",
    "In Q-Learning scenarios the agent will attempt EVERY discrete action from the action space at each state of the environment to form what is known as the Q-table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning - A codeless example\n",
    "\n",
    "Let's call our unicorn \"Agent Unicorn\" for consistency and let's say he's male. \n",
    "\n",
    "Given the environment in the image below - a simple 5X5 grid world, just like Taxi-v3, Agent Unicorn must reach the oats without entering any of the danger zones denoted by the Voldemort and foul weather. \n",
    "\n",
    "Agent Unicorn:\n",
    "\n",
    "* Can only move 1 tile per time step. This is a discrete action.\n",
    "* Can only move Up, Down, Left or Right. This is the Action Space.\n",
    "* Receives a -1 reward for each timestep. This encourages him to take the shortest path.\n",
    "* Receives a reward of +100 when it reaches the oats and the episode ends in a \"win\".\n",
    "* The danger zones are stationary and remain in the same tiles for all episodes\n",
    "* If he enters a danger zone, he receives a reward of -100 and the episode ends in a \"loss\".\n",
    "\n",
    "<img src=\"images/OatsGrid.png\" alt=\"enviro1\" width=\"400\" />\n",
    "\n",
    "Over many episodes and timesteps of \"learning\" the agent will explore the reward signal for each action at each state. \n",
    "Upon completion of this \"training\", the agent will know the reward signal for each state in the environment, shown in the image below.\n",
    "\n",
    "<img src=\"images/OatsGridReward.png\" alt=\"enviro1reward\" width=\"400\" />\n",
    "\n",
    "Reinforcement learning is meant to maximize the reward signal not just determine the reward signal for each state of the environment. Knowing each states reward signal and choosing a route to the oats that will MAXIMIZE the reward are two different things. This is where the Q-table can help us. The Q-table allows to calculate the maximum expected future reward by storing the reward for each action in each state (or tile).\n",
    "The image below represents all available actions in each state of our environment. Red Xs represent moves that are disallowed and are the boudaries to the environment.\n",
    "\n",
    "<img src=\"images/enviro1_actionspace.png\" alt=\"enviro1actionspace\" width=\"400\" />\n",
    "\n",
    "The Q-table will consist of one row for each state in the environment. Since our example is a 5X5 gridworld, and the non-agent objects (oats, Voldemort, etc.) do not move between episodes, our Q-table will have (5 x 5 = 25) rows. The Q-table will have a column for each action in the action space.  For our example then, our Q-table will have 4 columns.\n",
    "\n",
    "Here is what two rows from our Q-table would look like after the Q-Learing algorithm has fully populated them:\n",
    "\n",
    "<img src=\"images/stateToQtable1.png\" alt=\"stateToQtable\" width=\"800\"/>\n",
    "\n",
    "These are simply two states sampled from our unicorn's environment. An actual Q-table, once fully populated will have entries for each state in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Q-learning Algorithm\n",
    "\n",
    "Now that you have had an introduction to the Q-table, we will move on to the Q-learning <b>algorithm</b>. This algorithm contains the steps that we will use to populate the values in the Q-table.  \n",
    "\n",
    "* When our training begins we initialize an empty Q-table.  \n",
    "* Each entry in the Q-table is set to zero.\n",
    "\n",
    "### Now we begin the agent-environment interaction\n",
    "\n",
    "* Our agent makes an observation and chooses an action\n",
    "* The agent takes the action\n",
    "* Our agent receives the reward\n",
    "* The Q-table is updated to refelect the reward received at the respective state predicated on the respective action\n",
    "\n",
    "## The Q-learning Algorithm\n",
    "\n",
    "<img src=\"images/symbols/q_action_value_function.png\" alt=\"actvalfunc\" width=\"400\" />\n",
    "\n",
    "## The Q-learning Algorithm explained\n",
    "\n",
    "<img src=\"images/q_algo_explained.png\" alt=\"qalgoExpl\" width=\"400\" />\n",
    "\n",
    "All the elements of this algorithm should be familiar except one.  We have introduced a \"discount\" to our equation.\n",
    "\n",
    "<img src=\"images/symbols/gamma.png\" alt=\"gamma\" height=\"75\" width=\"75\"/>\n",
    "\n",
    "The discount rate determines the <i>present</i> value of <i>future</i> rewards. A reward received <i>k</i> timesteps into the future is worth only: $\\gamma^{k-1}$ times what it would be worth if it were received immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Epsilon and the Bellman Equation\n",
    "\n",
    "Let's talk about two more components of Q-learning (and many other forms of RL) and then we will get to some code!\n",
    "\n",
    "### Greedy Epsilon\n",
    "\n",
    "Greedy epsilon is the name given to the concept of balancing Exploration vs Exploitation. If our agent knows the value of taking an action in a given state, it can **expolit** that knowledge.  But what if our agent does not know the value of taking an action at a given state?  Then it must **explore**. We call it *greedy* because we want to choose the option that maximizes our reward signal.\n",
    "\n",
    "We use $\\epsilon$ (epsilon) to represent the value because $\\epsilon$ is a small positive infinitesimal quantity, whose limit is usually taken as $\\epsilon\\to0$ and our $\\epsilon$ value is incremented between 0 and 1.\n",
    "\n",
    "When we start training, we being with a high $\\epsilon$ to encourage exploration.  As more and more of the environment is explored we reduce $\\epsilon$ and begin to exploit where advantageous.\n",
    "\n",
    "<img src=\"images/expvsexp.png\" alt=\"expvsexp\" width=\"600\" />\n",
    "\n",
    "### The Bellman Equation \n",
    "\n",
    "The Bellman Equation is the means by which we update existing values in our Q-table.  Recall that the table is initialized with zeros.  Also recall that the reward at a given state is predicated on current AND future actions of our agent.\n",
    "\n",
    "<img src=\"images/symbols/bellmaneq.png\" alt=\"bellman\" width=\"600\" />\n",
    "\n",
    "The Bellman equation explained:\n",
    "\n",
    "<img src=\"images/bellmanexplained.png\" alt=\"bellmanexp\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some code!  Lets import our librariesâ€¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the entire notebook once and see what results you get. Then, and only then, go back to the hyperparameters. Take a look at what they are and what they do. Make some modifications and run again. Try a few different settings. Just like in golf: the lowest score wins!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='letsBegin'></a> <h1>Let's Begin</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym                           # The toolkit for developing and comparing reinforcement learning algorithms. \n",
    "import numpy as np                   # Matrix Multiplication library (our Q-table is a matrix)\n",
    "import sys                           # needed to cleanup the output statements from training\n",
    "import random                        # needed to generate random numbers\n",
    "import matplotlib.pyplot as plt      # Needed for graphing our agents training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym\n",
    "\n",
    "The Gym library greatly simplifies the process of developing RL algorithms. In this notebook we will work with the built-in \"toy-text\" environment called \"Taxi-v3\". Gym has many built in libraries and is fully open sourced so it can be extended (to use Atari ROMs as environments for example). Gym maps our action commands to the movements of the agent within the gym environment. Taxi-v3 is a small action space, but environments can have many more than 4 discrete actions. Gym provides the ability to pick a random action, provides episode and state management. Gym tells us if our agent as \"solved\" the environment or if it has expired. It is highly recommended you familiarize yourself with the [sourcecode](ps://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py) for the environment you are working with. You will find the discrete actions available, rendering and reward details, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and prepare the Environment\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# If you would like to see all of Gym's available environments you can uncomment\n",
    "# and run these two lines: from gym import envs for enviro in\n",
    "# envs.registry.all(): print(enviro)\n",
    "\n",
    "# Store action space and state_size.\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "# Let's take a look at our environment:\n",
    "env.render()\n",
    "\n",
    "# Note that Taxi-v3 is a simple ascii based environment.  Some environments and\n",
    "# toolkits may require graphics capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Map Key\n",
    "\n",
    "print(\"Blue Letter = unicorn\")\n",
    "print(\"Magenta Letter = magic oats\")\n",
    "print(\"Yellow = not hungry\")\n",
    "print(\"Green = hungry\")\n",
    "print(\"Letters = all possible oats locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our empty Q-table\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "print(\"Q-table initialized with {} states and {} discrete actions available per state\".format(state_size, action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Qtable is a matrix with 1 row for each possible state, and 1 column for each discrete action. If the taxi environment is a 5x5 ascii grid, why does our Q table have 500 states?\n",
    "\n",
    "In our oat hunting example above, the Voldemorts where stationary. We also made the assumption our agent would begin each episode in the same starting state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at a sample environment (it will look slightly different\n",
    "# rendered in a notebook than at a console)\n",
    "env.render()\n",
    "\n",
    "# remember the meanings of the colors\n",
    "# where is the unicorn?\n",
    "# where is the drop off point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to set a few variables now in order to setup training for our agent\n",
    "\n",
    "total_train_episodes = 10000    # How many times do we attempt to train on the Taxi environment (known as an episode)\n",
    "total_test_episodes = 10        # Number of episodes to test our qtable (should only take a few to verify maximized reward)\n",
    "learning_rate = 0.7             # Learning rate for Bellman equation\n",
    "gamma = 0.618                   # Gamma rate for the Bellman equation\n",
    "max_steps = 100                 # How many steps do we limit each episode to\n",
    "current_steps = 0               # Holder for the current step of the current training episode\n",
    "total_reward = 0                # Holder for the Cumulative reward of the current episode\n",
    "\n",
    "# Exploration parameter\n",
    "epsilon = 1                     # Greedy Epsilon\n",
    "max_epsilon = 1                 # Maximum Epsilon value\n",
    "min_epsilon = 0.1               # Minimum epsilon value\n",
    "decay_rate = .001               # Amount to reduce epsilon value after each episode.\n",
    "\n",
    "# Some variables to help us review our agents training progress\n",
    "epsilontrackers = []            # List to hold each epsilon value change for future viewing\n",
    "explore_counter = 0             # Counter that increments with each choice to explore\n",
    "explore_tracker = []            # list to hold the decision to explore\n",
    "exploit_counter = 0             # Counter that increments with each choice to exploit\n",
    "exploit_tracker = []            # List to hold the decision to exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's finally time to train\n",
    "\n",
    "# The agent-environment interaction:\n",
    "# 1. Make an observation and receive state information from the environment.\n",
    "# 2. Choose an action predicated on the data within state and propensity to explore or exploit.\n",
    "# 3. execute the action and receive the reward, update the Q-Table.\n",
    "        \n",
    "for episode in range(total_train_episodes + 1):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    for step in range(max_steps):\n",
    "        if (episode % 1000 ) == 0:\n",
    "            print(f\"\\r{episode} of {total_train_episodes} total episodes\", end=\"\")\n",
    "        \n",
    "        # Step One \n",
    "        # Make an Observation/ receive state information from the environment \n",
    "        # Because we are in a for loop, the first (step one) observation/state info has been \n",
    "        # established by the \"state = env.reset()\" code above, prior to entering this loop        \n",
    "        \n",
    "        # Step Two \n",
    "        # Choose an action predicated on the data within state and propensity to\n",
    "        # explore or exploit First we create a random number between 0 and 1\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "\n",
    "        # If the random number created above is greater than our current epsilon\n",
    "        # value -> we choose to exploit \"Exploit\" means to be \"greedy\" and take\n",
    "        # the max value for this state from the Q-table In the early timesteps\n",
    "        # the value of epsilon is very high - so this random number is unlikely\n",
    "        # to exceed epsilon\n",
    "        if exp_exp_tradeoff > epsilon:  \n",
    "            action = np.argmax(qtable[state, :])  # <-- Exploit\n",
    "            exploit_counter += 1\n",
    "        else: \n",
    "            action = env.action_space.sample()    # <-- Explore\n",
    "            explore_counter += 1\n",
    "        \n",
    "        # Update our explore vs exploit trackers\n",
    "        explore_tracker.append(explore_counter)\n",
    "        exploit_tracker.append(exploit_counter)\n",
    "\n",
    "        # Step Three \n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r) \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Probably the most important line of this example - our Bellman equation\n",
    "        # Recall the equation -> Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma *np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        # Make an Observation/ receive state information from the environment \n",
    "        # because we are in a for-loop, the following command can be considered part of Step One\n",
    "        state = new_state  \n",
    "\n",
    "        # What was once the new or next state is now the current state and the\n",
    "        # agent-environment loop repeats\n",
    "\n",
    "        # If done : finish episode\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # capture the current epsilon\n",
    "    epsilontrackers.append(epsilon)\n",
    "\n",
    "    # Each new epsiode - reduce epsilon by our chosen decay rate\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "\n",
    "print(f'Training complete! {episode} of {total_train_episodes} episodes completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once training is complete we will run the agent through the environment to\n",
    "# determine if it can maximize the reward. We simply repeat the process we used\n",
    "# to train, but rather than choosing to exploit or explore - we will use our\n",
    "# qTable to navigate the environment\n",
    "\n",
    "# An array to store our cumulative rewards to determine average 'reward per\n",
    "# episode'\n",
    "rewards = []\n",
    "\n",
    "# An array to store cumulative steps needed per episode to determine average\n",
    "# 'steps per episode'  \n",
    "steps = []\n",
    "\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()  # Always start with a fresh environment (we are clearing state data here)\n",
    "    step = 0             # reset episode step counter\n",
    "    done = False         # reset the episode over (done) flag\n",
    "    total_rewards = 0    # reset the episode cumulative reward counter\n",
    "\n",
    "    # The agent-environment interaction:\n",
    "    # 1. Make an Observation/ receive state information from the environment\n",
    "    # 2. Choose an action predicated on the data within state and propensity to\n",
    "    #    explore or exploit\n",
    "    # 3. Execute the action and receive the reward, update the qTable\n",
    "        \n",
    "    # max_steps is arbitrary - it could be set to 1,000,000 but if our agent has\n",
    "    # maximized the reward signal the episode will end in very few steps.\n",
    "    for step in range(max_steps): \n",
    "        \n",
    "        # Visualize the environment\n",
    "        # To limit scrolling in the output, we are only rendering the steps in the first episode\n",
    "        if episode is 0: \n",
    "            env.render()\n",
    "        \n",
    "        # Step One \n",
    "        # Make an Observation/ receive state information from the environment. Because\n",
    "        # we are within a for loop, the first (step one) observation\\state info has been\n",
    "        # established by the \"state = env.reset()\" code above, prior to entering this loop.\n",
    "        \n",
    "        # Step Two \n",
    "        # Choose an action that will maximize expected future reward  \n",
    "        # In our case will will choose the max value from the row in the QTable\n",
    "        # corresponding to our current state\n",
    "        action = np.argmax(qtable[state, :])\n",
    "\n",
    "        # Step Three \n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        total_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            steps.append(step)\n",
    "            print(\"Episode {} complete!\".format(episode))\n",
    "            print(\"Completed in {} steps with a reward of {}\".format(step, total_rewards))\n",
    "            break\n",
    "        \n",
    "        # Step One \n",
    "        # Make an Observation/ receive state information from the environment\n",
    "        # because we are in a for-loop, the following command can be considered\n",
    "        # part of Step One\n",
    "        \n",
    "        state = new_state\n",
    "        # What was once the new or next state is now the current state and the\n",
    "        # agent-environment loop repeats\n",
    "        \n",
    "env.close()\n",
    "\n",
    "print(\"\\nAll test episodes complete:\")\n",
    "print(\"Average number of steps per episode: {} average reward [score]: {}\".format(sum(steps)/total_test_episodes, \n",
    "                                                                                      sum(rewards)/total_test_episodes))\n",
    "\n",
    "# In the output window below you will see a rendering of each step of the first\n",
    "# episode so you can track the starting point and progress of your unicorn through\n",
    "# the environment as it seeks magic oats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stored a bit of information while we trained, lets review that now. Let's look at the rate of epsilon reduction in our greedy-epsilon implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(epsilontrackers)\n",
    "plt.ylabel('Epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the the agent's choice to explore or exploit.  \n",
    "Explore = blue\n",
    "Exploit = green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(explore_tracker, 'b', exploit_tracker, 'g')\n",
    "plt.annotate('less exploring and more exploiting', xy=(112000, 50000), xytext=(125000, 125000),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "            )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit the qTable and see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's left to do?  Here are some questions to think about while reflecting on this notebook:\n",
    "\n",
    "* If you wanted to persist this agent to use later - how would you save it?\n",
    "* We can optimize training with a more accurate value for the number of episodes required to train - how do we get a more accurate estimate for number of epsiodes needed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "Back to the games."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
