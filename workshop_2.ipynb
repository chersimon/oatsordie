{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <u>Reinforcement learning</u>:: Deep Q-learning</center>\n",
    "\n",
    "<img src=\"images/nb2warning.png\" alt=\"warning\" />\n",
    "\n",
    "<h1><center>What will you do in this Notebook?</center></h1>\n",
    "<br>\n",
    "\n",
    "### 1.) You will learn what Deep Q-learning is\n",
    "-- (the qTable is still with us, but we update it in a much more efficient manner.)\n",
    "\n",
    "\n",
    "### 2.) You will learn about the \"Gym-Retro\" \n",
    "-- <a href=\"https://github.com/openai/retro\">Gym-Retro</a> is a wrapper for video game emulator cores using the Libretro API to turn them into Gym environments. It includes support for several classic game consoles and a dataset of different games. <br/>\n",
    "-- Built by AI research company <a href=\"https://openai.com/\">OpenAI</a> <br/>\n",
    "\n",
    "\n",
    "### 3.) You will train an Agent in the fine art of playing Space Invaders\n",
    "<br/>\n",
    "<center><img src=\"images/spaceinvaders.png\" alt=\"spaceinvaders\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center>Deep Q-Learning </center></h1>\n",
    "\n",
    "<p>The <b>Q</b> stands for quality.  Or, what is the <i><b>quality of the reward signal<b></i> for being in a particular state.  Recall that our agent's goal is <b><i>Maximize the reward signal</i></b> (the Reward Hypothesis).  Each state in the environment will have a reward attached to it.  This reward can be positive or negative.</p> \n",
    "\n",
    "<p> These (Q)values are stored in a qTable </p>\n",
    "    \n",
    "<p>In the previous notebook we used a greedy epsilon ($\\gamma$) strategy to balance the choice of exploration vs exploitation.</p>\n",
    "\n",
    "<p>We will do the same here, and that just about ends the similarities.</p>\n",
    "\n",
    "<h3> DQN (<b>D</b>eep <b>Q</b> <b>N</b>etworks)</h3>\n",
    "<p> The orginal annoucement of the DQN came in 2015.  I highly encourage you to read the entire paper (AFTER the workshop).  It was created by DeepMind, creators of AlphaGo and the recently announced AlphaZero. The paper is <a href=\"https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/\">here</a>\n",
    "\n",
    "<h3> What's new with DQNs?</h3>\n",
    "- In the previous exercise the environment (Taxi-v2) was an <i><b>Episodic</b></i> environment. This means the task lasts a finite amount of time.  In Taxi-v2, when we drop off the passenger, the Episode ends.\n",
    "\n",
    "- <b>DQN</b> Are applicable towards <i><b>Continuous</b></i> environments.  Space Invaders does not last a finite amount of time. Each level simply increases the speed at which the invaders attack.  Granted, the game has a built in limit to say \"You have won\", but theoretically, it could just continue endlessly.\n",
    "\n",
    "- In the previous environment (Taxi-v2) we had a state space of 500 distinct states.  In a environment like Space Invaders we have potentially millions.  This means the qtable will no longer suffice to hold the maximum expected future reward.\n",
    "\n",
    "- A DQN will use a neural network, with the current state of the environment as the input to estimate the next best action.\n",
    "<br/>\n",
    "<img src=\"images/networklayers.png\" alt=\"network\"/>\n",
    "image from <a href=\"https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\">DeepMind</a>\n",
    "\n",
    "\n",
    "<img src =\"images/newconcept.png\" alt=\"newconcept\"/></div>\n",
    "<br/>\n",
    "<center><h3>New Concepts with DQNs</h3><center>\n",
    "<br/>\n",
    "1. DQN's introduced the concept of training only from pixels + game score from the environments\n",
    "\n",
    "2. The continuous environment and fast paced game place introduced the <b>\"Temporal Limitation\"</b> problem.\n",
    "> - With fast moving objects, the next move needs to take the agent to where the action <i>will be</i>, not where it is currently at.  In the lecture at the begining of this workshop\n",
    "> - This was addressed by introducing the concept of <i><b>Frame Stacking</b></i>. Or, taking several frames in succession as a single vector input.  This gives the agent enough information to ascertain the direction objects are moving in.\n",
    "<img src=\"images/framestacking.gif\" alt=\"framestacking\"/>\n",
    "<center>An example of 3 successive frames stacked together</center>\n",
    "\n",
    "3. DQN's introduced the concept of experience replay - partly because it was required to give the NN something to learn from, and partly because it was discovered that when using a NN, we no longer adehered to MDP Dynamic Programming and could \"Remember\" similar states from the past.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Gym-Retro</h3></center>\n",
    "Gym-Retro is a wrapper for video game emulator cores using the Libretro API to turn them into Gym environments. It includes support for several classic game consoles and a dataset of different games.\n",
    "\n",
    "- How does it emulate these games?\n",
    "> - Each game has files listing memory locations for in-game variables, reward functions based on those variables, episode end conditions, savestates at the beginning of levels and a file containing hashes of ROMs that work with these files.\n",
    "\n",
    "- What game systems can it emulate?\n",
    "> - Atari2600 (via Stella)\n",
    "> - NEC TurboGrafx-16/PC Engine (via Mednafen/Beetle PCE Fast)\n",
    "> - Nintendo Game Boy/Game Boy Color (via gambatte)\n",
    "> - Nintendo Game Boy Advance (via mGBA)\n",
    "> - Nintendo Entertainment System (via FCEUmm)\n",
    "> - Super Nintendo Entertainment System (via Snes9x)\n",
    "> - Sega GameGear (via Genesis Plus GX)\n",
    "> - Sega Genesis/Mega Drive (via Genesis Plus GX)\n",
    "> - Master System (via Genesis Plus GX)\n",
    "\n",
    "- You can record your game (training session with Retro) by including the 'record' parameter in the environment instantiation\n",
    "> - env = retro.make(game='AirStriker-Genesis', record='.')\n",
    "\n",
    "\n",
    "<img src=\"https://atariage.com//images/query_headers/Atari2600_Screenshots_Header.jpg\" alt=\"atari\"/>\n",
    "image: some completely random site I found on the internet that you would <b>NOT</b> want to go to if you needed <a href=\"https://atariage.com/system_items.html?SystemID=2600&ItemTypeID=ROM\">Atari</a> ROM files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get our libraries imported\n",
    "import retro                         # Gym-Retro library from OpenAI to manage the Atari emulation and RL environment\n",
    "import tensorflow as tf              # If you don't know what TF is, we can't be friends.\n",
    "import numpy as np                   # Matrix Multiplication\n",
    "import matplotlib.pyplot as plt      # Needed for graphing our agents training progress\n",
    "import random                        # needed to generate random numbers\n",
    "from skimage import transform        # Image manipulation library\n",
    "from skimage.color import rgb2gray   # Convert RGB images to grayscale\n",
    "from collections import deque        # A FIFO queue - pronounced - DECK\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Instantiate and explore (human exploration, not agent exploration) the environment \n",
    "#env = retro.make(game='SpaceInvaders-Atari2600')\n",
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at the environment\n",
    "sample_state = env.reset()\n",
    "env.render()\n",
    "print(\"Frame Shape:\", env.observation_space)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
